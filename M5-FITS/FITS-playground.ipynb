{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1u7dW2YhfWTBGX-L3w-0185yYaRiih1vs","timestamp":1732529820209}],"authorship_tag":"ABX9TyOpQYYZnC05yB+7WvGb5tGc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AkvzEz1lubvJ","executionInfo":{"status":"ok","timestamp":1733260151389,"user_tz":-120,"elapsed":88601,"user":{"displayName":"Mantas Bagdonas","userId":"12975773294315109710"}},"outputId":"da2b1975-69be-447e-b574-03a56b32ef5e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["root_path = '/content/drive/MyDrive/M5-FITS/processed-nonparam/'\n","fits_dir = '/content/drive/MyDrive/FITS'"],"metadata":{"id":"mxkYRiTc3Q2S","executionInfo":{"status":"ok","timestamp":1733260152822,"user_tz":-120,"elapsed":2,"user":{"displayName":"Mantas Bagdonas","userId":"12975773294315109710"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import os, sys, gc, time, warnings, pickle, psutil, random\n","\n","os.chdir(fits_dir)\n","from models.FITS import Model\n","# os.chdir(fits_dir + '/data_provider')\n","# from data_provider.data_factory import data_provider"],"metadata":{"id":"-4VosJGi7fRw","executionInfo":{"status":"ok","timestamp":1733260159305,"user_tz":-120,"elapsed":6485,"user":{"displayName":"Mantas Bagdonas","userId":"12975773294315109710"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","import os\n","import torch\n","import random\n","from torch.utils.data import Dataset\n","from sklearn.preprocessing import StandardScaler\n","\n","warnings.filterwarnings('ignore')\n","\n","class Config(object):\n","  def __init__(self, args):\n","    #basic config\n","    self.is_training = args.get('is_training', 1)\n","    self.model_id = args.get('model_id', 'test')\n","    self.model = args.get('model', 'Autoformer')\n","    #dataloader\n","    self.data = args.get('data', 'test')\n","    self.root_path = args.get('root_path', '/content/drive/MyDrive/M5-FITS/processed-nonparam')\n","    self.data_path = args.get('data_path', 'm5.csv')\n","    self.features = args.get('features', 'M')\n","    self.target = args.get('target', 'sales')\n","    self.freq = args.get('freq', 'd')\n","    self.checkpoints = args.get('checkpoints', '/content/drive/MyDrive/M5-FITS/checkpoints')\n","    #forecasting\n","    self.seq_len = args.get('seq_len', 56)\n","    self.pred_len = args.get('pred_len', 28)\n","    self.label_len = args.get('label_len', 28)\n","    self.individual = args.get('individual', False)\n","    #optimization\n","    self.num_workers = args.get('num_workers', 10)\n","    self.itr = args.get('itr', 2)\n","    self.train_epochs = args.get('train_epochs', 100)\n","    self.batch_size = args.get('batch_size', 32)\n","    self.patience = args.get('patience', 3)\n","    self.learning_rate = args.get('learning_rate', 0.0001)\n","    self.des = args.get('des', 'test')\n","    self.loss = args.get('loss', 'mse')\n","    self.lradj = args.get('lradj', 'type3')\n","    self.use_amp = args.get('use_amp', False)\n","    #GPU\n","    self.use_gpu = args.get('use_gpu', True)\n","    self.gpu = args.get('gpu', 0)\n","    self.use_multi_gpu = args.get('use_multi_gpu', False)\n","    self.devices = args.get('devices', '0,1,2,3')\n","    self.test_flop = args.get('test_flop', False)\n","    #Augmentation\n","    self.aug_method = args.get('aug_method', 'NA')\n","    self.aug_rate = args.get('aug_rate', 0.5)\n","    self.in_batch_augmentation = args.get('in_batch_augmentation', False)\n","    self.in_dataset_augmentation = args.get('in_dataset_augmentation', False)\n","    self.data_size = args.get('data_size', 1)\n","    self.aug_data_size = args.get('aug_data_size', 1)\n","    self.seed = args.get('seed', 2021)\n","    #continue learning\n","    self.testset_div = args.get('testset_div', 2)\n","    self.test_time_train = args.get('test_time_train', False)\n","    #Formers\n","    self.embed = args.get('embed', 'timeF')\n","    self.enc_in = args.get('enc_in', 7)\n","    self.dec_in = args.get('dec_in', 7)\n","    self.c_out = args.get('c_out', 7)\n","    self.d_model = args.get('d_model', 512)\n","    self.n_heads = args.get('n_heads', 8)\n","    self.e_layers = args.get('e_layers', 2)\n","    self.d_layers = args.get('d_layers', 1)\n","    self.d_ff = args.get('d_ff', 2048)\n","    self.moving_avg = args.get('moving_avg', 25)\n","    self.factor = args.get('factor', 1)\n","    self.distil = args.get('distil', True)\n","    self.dropout = args.get('dropout', 0.1)\n","    self.activation = args.get('activation', 'relu')\n","    self.output_attention = args.get('output_attention', False)\n","    self.do_predict = args.get('do_predict', False)\n","\n","    #Flinear\n","    self.train_mode = args.get('train_mode', 0)\n","    self.cut_freq = args.get('cut_freq', 0)\n","    self.base_T = args.get('base_T', 24)\n","    self.H_order = args.get('H_order', 2)\n","\n","    self.use_gpu = True if torch.cuda.is_available() and self.use_gpu else False\n","    cfreq = args.get('cut_freq', 0)\n","    if cfreq == 0:\n","      self.cut_freq = int(self.seq_len // self.base_T + 1) * self.H_order + 10\n","\n","    fix_seed = self.seed\n","    random.seed(fix_seed)\n","    torch.manual_seed(fix_seed)\n","    np.random.seed(fix_seed)"],"metadata":{"id":"vs9vl9WunyEu","executionInfo":{"status":"ok","timestamp":1733261626245,"user_tz":-120,"elapsed":2560,"user":{"displayName":"Mantas Bagdonas","userId":"12975773294315109710"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def file_name(store_id, dept_id):\n","  return 'data_train_' + store_id + '_' + dept_id + '.csv'"],"metadata":{"id":"XmM9tUcVIOdO","executionInfo":{"status":"ok","timestamp":1733261626245,"user_tz":-120,"elapsed":2,"user":{"displayName":"Mantas Bagdonas","userId":"12975773294315109710"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["args = {\n","    'root_path': '/content/drive/MyDrive/M5-FITS/processed/v1',\n","    'data': 'custom',\n","    # 'data_path': 'df_train_TX_1_HOUSEHOLD_1.csv',\n","    'features': 'S',\n","    'model': 'FITS'\n","}\n","config = Config(args)"],"metadata":{"id":"X9Z5XePivSQl","executionInfo":{"status":"ok","timestamp":1733261626245,"user_tz":-120,"elapsed":2,"user":{"displayName":"Mantas Bagdonas","userId":"12975773294315109710"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["store_id = 'CA_1'\n","dept_id = 'HOBBIES_1'"],"metadata":{"id":"4mrteCxbHdHR","executionInfo":{"status":"ok","timestamp":1733261626245,"user_tz":-120,"elapsed":2,"user":{"displayName":"Mantas Bagdonas","userId":"12975773294315109710"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["config.data_path = file_name(store_id, dept_id)"],"metadata":{"id":"bkaJjkjSHetn","executionInfo":{"status":"ok","timestamp":1733261626245,"user_tz":-120,"elapsed":1,"user":{"displayName":"Mantas Bagdonas","userId":"12975773294315109710"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["shuffle_flag = True\n","drop_last = True\n","batch_size = config.batch_size\n","freq = config.freq\n","timeenc = 0 if config.embed != 'timeF' else 1"],"metadata":{"id":"SIpU-fQLIV0v","executionInfo":{"status":"ok","timestamp":1733261626245,"user_tz":-120,"elapsed":1,"user":{"displayName":"Mantas Bagdonas","userId":"12975773294315109710"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","from utils.timefeatures import time_features\n","\n","TRAIN_START = 0\n","TRAIN_END = 1941\n","TEST_END = 1969\n","TRAIN_LEN = TRAIN_END - TRAIN_START\n","HORIZON = 28\n","\n","class Dataset_Custom(Dataset):\n","    def __init__(self, config, root_path, flag='train', size=None,\n","                 features='S', data_path='ETTh1.csv',\n","                 target='OT', scale=True, timeenc=0, freq='h'):\n","        self.args = config\n","        # info\n","        if size == None:\n","            self.seq_len = 24 * 4 * 4\n","            self.label_len = 24 * 4\n","            self.pred_len = 24 * 4\n","        else:\n","            self.seq_len = size[0]\n","            self.label_len = size[1]\n","            self.pred_len = size[2]\n","        # init\n","        assert flag in ['train', 'test', 'val']\n","        type_map = {'train': 0, 'val': 1, 'test': 2}\n","        self.set_type = type_map[flag]\n","\n","        self.features = features\n","        self.target = target\n","        self.scale = scale\n","        self.timeenc = timeenc\n","        self.freq = freq\n","\n","        self.root_path = root_path\n","        self.data_path = data_path\n","        self.__read_data__()\n","        self.collect_all_data()\n","        if self.args.in_dataset_augmentation and self.set_type==0:\n","            self.data_augmentation()\n","\n","    def __read_data__(self):\n","        self.scaler = StandardScaler()\n","        #TODO: read bottom-level data. Reproduce similar functionality as in DLinear\n","        df_raw = pd.read_csv(os.path.join(self.root_path,\n","                                          self.data_path))\n","        '''\n","        df_raw.columns: ['date', ...(other features), target feature]\n","        '''\n","        cols = list(df_raw.columns)\n","        cols.remove(self.target)\n","        cols.remove('date')\n","        df_raw = df_raw[['date'] + cols + [self.target]]\n","        # print(cols)\n","        # num_train = int(len(df_raw) * 0.7)\n","        # num_test = int(len(df_raw) * 0.2)\n","        num_test = HORIZON  # Fixed to the last 28 days\n","        num_train = int((TRAIN_END - TRAIN_START) * 0.8)\n","        num_vali = TRAIN_END - TRAIN_START - num_train - num_test\n","        border1s = [0, num_train - self.seq_len, len(df_raw) - num_test - self.seq_len]\n","        border2s = [num_train, num_train + num_vali, len(df_raw)]\n","\n","        if self.args.test_time_train:\n","            num_train = int(len(df_raw) * 0.9)\n","            border1s = [0, num_train - self.seq_len, len(df_raw)]\n","            border2s = [num_train, len(df_raw), len(df_raw)]\n","\n","        border1 = border1s[self.set_type]\n","        border2 = border2s[self.set_type]\n","\n","        if self.features == 'M' or self.features == 'MS':\n","            cols_data = df_raw.columns[1:]\n","            df_data = df_raw[cols_data]\n","        elif self.features == 'S':\n","            df_data = df_raw[[self.target]]\n","\n","        if self.scale:\n","            train_data = df_data[border1s[0]:border2s[0]]\n","            self.scaler.fit(train_data.values)\n","            # print(self.scaler.mean_)\n","            # exit()\n","            data = self.scaler.transform(df_data.values)\n","        else:\n","            data = df_data.values\n","\n","        df_stamp = df_raw[['date']][border1:border2]\n","        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n","        if self.timeenc == 0:\n","            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n","            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n","            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n","            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n","            data_stamp = df_stamp.drop(['date'], 1).values\n","        elif self.timeenc == 1:\n","            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n","            data_stamp = data_stamp.transpose(1, 0)\n","\n","        self.data_x = data[border1:border2]\n","        self.data_y = data[border1:border2]\n","        self.data_stamp = data_stamp\n","        print(border1, border2)\n","\n","    def regenerate_augmentation_data(self):\n","        self.collect_all_data()\n","        self.data_augmentation()\n","\n","    def reload_data(self, x_data, y_data, x_time, y_time):\n","        self.x_data = x_data\n","        self.y_data = y_data\n","        self.x_time = x_time\n","        self.y_time = y_time\n","\n","    def collect_all_data(self):\n","        self.x_data = []\n","        self.y_data = []\n","        self.x_time = []\n","        self.y_time = []\n","        data_len = len(self.data_x) - self.seq_len - self.pred_len + 1\n","        mask_data_len = int((1-self.args.data_size) * data_len) if self.args.data_size < 1 else 0\n","        for i in range(len(self.data_x) - self.seq_len - self.pred_len + 1):\n","            if (self.set_type == 0 and i >= mask_data_len) or self.set_type != 0:\n","                s_begin = i\n","                s_end = s_begin + self.seq_len\n","                r_begin = s_end - self.label_len\n","                r_end = r_begin + self.label_len + self.pred_len\n","                self.x_data.append(self.data_x[s_begin:s_end])\n","                self.y_data.append(self.data_y[r_begin:r_end])\n","                self.x_time.append(self.data_stamp[s_begin:s_end])\n","                self.y_time.append(self.data_stamp[r_begin:r_end])\n","\n","    def data_augmentation(self):\n","        origin_len = len(self.x_data)\n","        if not self.args.closer_data_aug_more:\n","            aug_size = [self.args.aug_data_size for i in range(origin_len)]\n","        else:\n","            aug_size = [int(self.args.aug_data_size * i/origin_len) + 1 for i in range(origin_len)]\n","\n","        for i in range(origin_len):\n","            for _ in range(aug_size[i]):\n","                aug = augmentation('dataset')\n","                if self.args.aug_method == 'f_mask':\n","                    x,y = aug.freq_dropout(self.x_data[i],self.y_data[i],dropout_rate=self.args.aug_rate)\n","                elif self.args.aug_method == 'f_mix':\n","                    rand = float(np.random.random(1))\n","                    i2 = int(rand*len(self.x_data))\n","                    x,y = aug.freq_mix(self.x_data[i],self.y_data[i],self.x_data[i2],self.y_data[i2],dropout_rate=self.args.aug_rate)\n","                else:\n","                    raise ValueError\n","                self.x_data.append(x)\n","                self.y_data.append(y)\n","                self.x_time.append(self.x_time[i])\n","                self.y_time.append(self.y_time[i])\n","\n","    def __getitem__(self, index):\n","        seq_x = self.x_data[index]\n","        seq_y = self.y_data[index]\n","        return seq_x, seq_y, self.x_time[index], self.y_time[index]\n","\n","    def __len__(self):\n","        return len(self.x_data)\n","\n","    def inverse_transform(self, data):\n","        return self.scaler.inverse_transform(data)\n","\n","def data_provider(args, flag):\n","  Data = Dataset_Custom\n","  timeenc = 0 if args.embed != 'timeF' else 1\n","\n","  if flag == 'test':\n","      shuffle_flag = False\n","      drop_last = False # True\n","      batch_size = args.batch_size\n","      freq = args.freq\n","  else:\n","      shuffle_flag = True\n","      drop_last = True\n","      batch_size = args.batch_size\n","      freq = args.freq\n","\n","  data_set = Data(\n","      config=args,\n","      root_path=args.root_path,\n","      data_path=args.data_path,\n","      flag=flag,\n","      size=[args.seq_len, args.label_len, args.pred_len],\n","      features=args.features,\n","      target=args.target,\n","      timeenc=timeenc,\n","      freq=freq\n","  )\n","\n","  data_loader = DataLoader(\n","      data_set,\n","      batch_size=batch_size,\n","      shuffle=shuffle_flag,\n","      num_workers=args.num_workers,\n","      drop_last=drop_last)\n","\n","  return data_set, data_loader\n","\n","\n","data_set = Dataset_Custom(\n","    config=config,\n","    root_path=config.root_path,\n","    data_path=config.data_path,\n","    flag='train',\n","    size=[config.seq_len, config.label_len, config.pred_len],\n","    features=config.features,\n","    target=config.target,\n","    timeenc=timeenc,\n","    freq=config.freq,\n","    scale=True\n",")\n","\n","print('train', len(data_set))\n","data_loader = DataLoader(\n","    data_set,\n","    batch_size=batch_size,\n","    shuffle=False,\n","    num_workers=config.num_workers,\n","    drop_last=drop_last)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XaNX12T8Ipc0","executionInfo":{"status":"ok","timestamp":1733261629168,"user_tz":-120,"elapsed":1576,"user":{"displayName":"Mantas Bagdonas","userId":"12975773294315109710"}},"outputId":"392391d9-baa5-425a-edc5-e56ad00ccb7c"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["0 1552\n","train 1469\n"]}]},{"cell_type":"code","source":["!pip3 install thop"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"PgmtBS7vQGYZ","executionInfo":{"status":"ok","timestamp":1733261632599,"user_tz":-120,"elapsed":3433,"user":{"displayName":"Mantas Bagdonas","userId":"12975773294315109710"}},"outputId":"caff00ed-0288-40c5-8ef6-7f54be60c6cf"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting thop\n","  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from thop) (2.5.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->thop) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->thop) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->thop) (3.0.2)\n","Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Installing collected packages: thop\n","Successfully installed thop-0.1.1.post2209072238\n"]}]},{"cell_type":"code","source":["from models.FITS import Model\n","from utils.tools import EarlyStopping, adjust_learning_rate, test_params_flop, visual\n","from utils.metrics import metric\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","from utils.augmentations import augmentation\n","import os\n","import time\n","\n","import warnings\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from thop import profile\n","\n","class FITS(object):\n","    def __init__(self, args):\n","        self.args = args\n","        self.device = self._acquire_device()\n","        self.model = self._build_model().to(self.device)\n","\n","    def _build_model(self):\n","        raise NotImplementedError\n","        return None\n","\n","    def _acquire_device(self):\n","        if self.args.use_gpu:\n","            os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(\n","                self.args.gpu) if not self.args.use_multi_gpu else self.args.devices\n","            device = torch.device('cuda:{}'.format(self.args.gpu))\n","            print('Use GPU: cuda:{}'.format(self.args.gpu))\n","        else:\n","            device = torch.device('cpu')\n","            print('Use CPU')\n","        return device\n","\n","    def _get_data(self):\n","        pass\n","\n","    def vali(self):\n","        pass\n","\n","    def train(self):\n","        pass\n","\n","    def test(self):\n","        pass\n","\n","class M5FITS(FITS):\n","    def __init__(self, args):\n","        super(M5FITS, self).__init__(args)\n","\n","    def _build_model(self):\n","        model = Model(self.args).float()\n","\n","        if self.args.use_multi_gpu and self.args.use_gpu:\n","            model = nn.DataParallel(model, device_ids=self.args.device_ids)\n","\n","        return model\n","\n","    def _get_data(self, flag):\n","        data_set, data_loader = data_provider(self.args, flag)\n","        return data_set, data_loader\n","\n","    def _select_optimizer(self):\n","        model_optim = optim.Adam(self.model.parameters(), lr=self.args.learning_rate)\n","        print('!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!')\n","        print(self.args.learning_rate)\n","        return model_optim\n","\n","    def _select_criterion(self):\n","        criterion = nn.MSELoss()\n","        return criterion\n","\n","    def _get_profile(self, model):\n","        _input=torch.randn(self.args.batch_size, self.args.seq_len, self.args.enc_in).to(self.device)\n","        macs, params = profile(model, inputs=(_input,))\n","        print('FLOPs: ', macs)\n","        print('params: ', params)\n","        return macs, params\n","\n","    def vali(self, vali_data, vali_loader, criterion):\n","        total_loss = []\n","        self.model.eval()\n","        with torch.no_grad():\n","            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):\n","                batch_x = batch_x.float().to(self.device)\n","                batch_y = batch_y.float().to(self.device)[:,-self.args.pred_len:,:]\n","                batch_xy = torch.cat([batch_x, batch_y], dim=1)\n","\n","                batch_x_mark = batch_x_mark.float().to(self.device)\n","                batch_y_mark = batch_y_mark.float().to(self.device)\n","\n","                # decoder input\n","                dec_inp = torch.zeros_like(batch_y[:, -self.args.pred_len:, :]).float()\n","                dec_inp = torch.cat([batch_y[:, :self.args.label_len, :], dec_inp], dim=1).float().to(self.device)\n","                # encoder - decoder\n","                if 'FITS' in self.args.model:\n","                    outputs, low = self.model(batch_x)\n","                elif 'SCINet' in self.args.model:\n","                    outputs = self.model(batch_x)\n","                else:\n","                    if self.args.output_attention:\n","                        outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n","                    else:\n","                        outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n","                f_dim = -1 if self.args.features == 'MS' else 0\n","                outputs = outputs[:, -self.args.pred_len:, f_dim:]\n","                batch_y = batch_y[:, -self.args.pred_len:, f_dim:]\n","\n","                pred = outputs.detach().cpu()\n","                true = batch_y.detach().cpu()\n","\n","                loss = criterion(pred, true)\n","\n","                total_loss.append(loss)\n","        total_loss = np.average(total_loss)\n","        self.model.train()\n","        return total_loss\n","\n","    def train(self, setting, ft=False):\n","        train_data, train_loader = self._get_data(flag='train')\n","        vali_data, vali_loader = self._get_data(flag='val')\n","        test_data, test_loader = self._get_data(flag='test')\n","        print(self.model)\n","        self._get_profile(self.model)\n","        print('Trainable parameters: ', sum(p.numel() for p in self.model.parameters() if p.requires_grad))\n","\n","        path = os.path.join(self.args.checkpoints, setting)\n","        if not os.path.exists(path):\n","            os.makedirs(path)\n","\n","        time_now = time.time()\n","\n","        train_steps = len(train_loader)\n","        early_stopping = EarlyStopping(patience=self.args.patience, verbose=True)\n","\n","        model_optim = self._select_optimizer()\n","        criterion = self._select_criterion()\n","\n","        for epoch in range(self.args.train_epochs):\n","            iter_count = 0\n","            train_loss = []\n","\n","            self.model.train()\n","            epoch_time = time.time()\n","            if self.args.in_dataset_augmentation:\n","                train_loader.dataset.regenerate_augmentation_data()\n","\n","            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n","                iter_count += 1\n","                model_optim.zero_grad()\n","\n","                batch_x = batch_x.float().to(self.device)\n","                batch_y = batch_y.float().to(self.device)[:,-self.args.pred_len:,:]\n","                batch_x_mark = batch_x_mark.float().to(self.device)\n","                batch_y_mark = batch_y_mark.float().to(self.device)\n","                # print(batch_x.shape, batch_y.shape)\n","                batch_xy = torch.cat([batch_x, batch_y], dim=1)\n","\n","                # if self.args.in_batch_augmentation:\n","                #     aug = augmentation('batch')\n","                #     methods = {'f_mask':aug.freq_mask, 'f_mix': aug.freq_mix, 'noise':aug.noise,'noise_input':aug.noise_input}\n","                #     for step in range(self.args.aug_data_size):\n","                #         xy = methods[self.args.aug_method](batch_x, batch_y[:, -self.args.pred_len:, :], rate=self.args.aug_rate, dim=1)\n","                #         batch_x2, batch_y2 = xy[:, :self.args.seq_len, :], xy[:, -self.args.label_len-self.args.pred_len:, :]\n","                #         if 'noise' not in self.args.aug_method:\n","                #             batch_x = torch.cat([batch_x,batch_x2],dim=0)\n","                #             batch_y = torch.cat([batch_y,batch_y2],dim=0)\n","                #             batch_x_mark = torch.cat([batch_x_mark,batch_x_mark],dim=0)\n","                #             batch_y_mark = torch.cat([batch_y_mark,batch_y_mark],dim=0)\n","                #         else:\n","                #             print('noise')\n","                #             batch_x = batch_x2\n","                #             batch_y = batch_y2\n","\n","                # decoder input\n","                dec_inp = torch.zeros_like(batch_y[:, -self.args.pred_len:, :]).float()\n","                dec_inp = torch.cat([batch_y[:, :self.args.label_len, :], dec_inp], dim=1).float().to(self.device)\n","\n","                # encoder - decoder\n","                if 'FITS' in self.args.model:\n","                        outputs, low = self.model(batch_x)\n","                elif 'SCINet' in self.args.model:\n","                        outputs = self.model(batch_x)\n","                else:\n","                    if self.args.output_attention:\n","                        outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n","                    else:\n","                        outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark, batch_y)\n","\n","                # print(outputs.shape,batch_y.shape)\n","                f_dim = -1 if self.args.features == 'MS' else 0\n","                if ft:\n","                    outputs = outputs[:, -self.args.pred_len:, f_dim:]\n","                    batch_y = batch_y[:, -self.args.pred_len:, f_dim:].to(self.device)\n","                    # print(outputs.shape,batch_xy.shape)\n","                    #loss = criterion(outputs, batch_xy)\n","                    loss = criterion(outputs, batch_y)\n","                else:\n","                    outputs = outputs[:, :, f_dim:]\n","                    # batch_y = batch_y[:, -self.args.pred_len:, f_dim:].to(self.device) #???\n","                    loss = criterion(outputs, batch_xy)\n","                train_loss.append(loss.item())\n","\n","                if (i + 1) % 100 == 0:\n","                    print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n","                    speed = (time.time() - time_now) / iter_count\n","                    left_time = speed * ((self.args.train_epochs - epoch) * train_steps - i)\n","                    print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n","                    iter_count = 0\n","                    time_now = time.time()\n","\n","                loss.backward()\n","                model_optim.step()\n","\n","            print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n","            train_loss = np.average(train_loss)\n","            vali_loss = self.vali(vali_data, vali_loader, criterion)\n","            test_loss = self.vali(test_data, test_loader, criterion)\n","\n","            print(\"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}\".format(\n","                epoch + 1, train_steps, train_loss, vali_loss, test_loss))\n","            early_stopping(vali_loss, self.model, path)\n","            if early_stopping.early_stop:\n","                print(\"Early stopping\")\n","                break\n","\n","            adjust_learning_rate(model_optim, epoch + 1, self.args)\n","\n","        best_model_path = path + '/' + 'checkpoint.pth'\n","        self.model.load_state_dict(torch.load(best_model_path))\n","\n","        return self.model\n","\n","    def test(self, setting, test=0):\n","        test_data, test_loader = self._get_data(flag='test')\n","\n","        if test:\n","            print('loading model')\n","            self.model.load_state_dict(torch.load(os.path.join('./checkpoints/' + setting, 'checkpoint.pth')))\n","\n","        preds = []\n","        trues = []\n","        inputx = []\n","        reconx = []\n","        inputxy = []\n","        reconxy = []\n","        lows = []\n","        folder_path = './test_results/' + setting + '/'\n","        if not os.path.exists(folder_path):\n","            os.makedirs(folder_path)\n","\n","        self.model.eval()\n","        with torch.no_grad():\n","            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n","                batch_x = batch_x.float().to(self.device)\n","                batch_y = batch_y.float().to(self.device)[:,-self.args.pred_len:,:]\n","                batch_xy = torch.cat([batch_x, batch_y], dim=1).float().to(self.device)\n","\n","                batch_x_mark = batch_x_mark.float().to(self.device)\n","                batch_y_mark = batch_y_mark.float().to(self.device)\n","\n","                # decoder input\n","                dec_inp = torch.zeros_like(batch_y[:, -self.args.pred_len:, :]).float()\n","                dec_inp = torch.cat([batch_y[:, :self.args.label_len, :], dec_inp], dim=1).float().to(self.device)\n","                # encoder - decoder\n","\n","                if 'FITS' in self.args.model:\n","                        outputs, low = self.model(batch_x)\n","                elif 'SCINet' in self.args.model:\n","                        outputs = self.model(batch_x)\n","                else:\n","                    if self.args.output_attention:\n","                        outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n","\n","                    else:\n","                        outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n","\n","                f_dim = -1 if self.args.features == 'MS' else 0\n","                # print(outputs.shape,batch_y.shape)\n","                outputs_ = outputs[:, -self.args.pred_len:, f_dim:]\n","                batch_y = batch_y[:, -self.args.pred_len:, f_dim:].to(self.device)\n","                outputs_ = outputs_.detach().cpu().numpy()\n","                batch_y = batch_y.detach().cpu().numpy()\n","\n","\n","                pred = outputs_  # outputs.detach().cpu().numpy()  # .squeeze()\n","                true = batch_y  # batch_y.detach().cpu().numpy()  # .squeeze()\n","\n","                preds.append(pred)\n","                trues.append(true)\n","                inputx.append(batch_x.detach().cpu().numpy())\n","                inputxy.append(batch_xy.detach().cpu().numpy())\n","                reconx.append(outputs[:, :-self.args.pred_len, f_dim:].detach().cpu().numpy())\n","                reconxy.append(outputs.detach().cpu().numpy())\n","                lows.append(low.detach().cpu().numpy())\n","                if i % 20 == 0:\n","                    input = batch_x.detach().cpu().numpy()\n","                    gt = np.concatenate((input[0, :, -1], true[0, :, -1]), axis=0)\n","                    pd = np.concatenate((input[0, :, -1], pred[0, :, -1]), axis=0)\n","                    visual(gt, pd, os.path.join(folder_path, str(i) + '.pdf'))\n","\n","        if self.args.test_flop:\n","            test_params_flop((batch_x.shape[1],batch_x.shape[2]))\n","            exit()\n","        preds = np.concatenate(preds, axis=0)\n","        trues = np.concatenate(trues, axis=0)\n","        # inputx = np.array(inputx)\n","        # reconx = np.array(reconx)\n","        # reconxy = np.array(reconxy)\n","        # inputxy = np.array(inputxy)\n","        # lows = np.array(lows)\n","\n","\n","        # preds = preds.reshape(-1, preds.shape[-2], preds.shape[-1])\n","        # trues = trues.reshape(-1, trues.shape[-2], trues.shape[-1])\n","        # inputx = inputx.reshape(-1, inputx.shape[-2], inputx.shape[-1])\n","        # reconx = reconx.reshape(-1, reconx.shape[-2], reconx.shape[-1])\n","        # reconxy = reconxy.reshape(-1, reconxy.shape[-2], reconxy.shape[-1])\n","        # inputxy = inputxy.reshape(-1, inputxy.shape[-2], inputxy.shape[-1])\n","        # lows = lows.reshape(-1, lows.shape[-2], lows.shape[-1])\n","\n","        # try:\n","        #     for i in range(0,2800,300):\n","\n","        #         # create a figure with 3 subplots\n","        #         fig, axs = plt.subplots(3, 1, figsize=(10, 10))\n","        #         # plot pred and true in the first subplot\n","        #         axs[0].plot(trues[i, :, -1], label='true')\n","        #         axs[0].plot(preds[i, :, -1], label='pred')\n","        #         axs[0].set_title('pred and true')\n","        #         # plot inputx and reconx in the second subplot\n","        #         axs[1].plot(inputx[i, :, -1], label='inputx')\n","        #         axs[1].plot(reconx[i, :, -1], label='reconx')\n","        #         axs[1].set_title('inputx and reconx')\n","        #         # plot inputxy and reconxy in the third subplot\n","        #         axs[2].plot(inputxy[i, :, -1], label='inputxy')\n","        #         axs[2].plot(reconxy[i, :, -1], label='reconxy')\n","        #         axs[2].plot(lows[i, :, -1])\n","        #         axs[2].set_title('inputxy and reconxy')\n","        #         # show the legend\n","        #         plt.legend()\n","        #         # save the figure to file\n","        #         fig.savefig(os.path.join(folder_path, str(i) + '_F.png'))\n","        #         # print('plottting')\n","        # except:\n","        #     pass\n","\n","        # result save\n","        # folder_path = './results/' + setting + '/'\n","        # if not os.path.exists(folder_path):\n","        #     os.makedirs(folder_path)\n","\n","        # mae, mse, rmse, mape, mspe, rse, corr = metric(preds, trues)\n","        # print('mse:{}, mae:{}, rse:{}, corr:{}'.format(mse, mae, rse, corr))\n","        # f = open(\"result.txt\", 'a')\n","        # f.write(setting + \"  \\n\")\n","        # f.write('mse:{}, mae:{}, rse:{}, corr:{}'.format(mse, mae, rse, corr))\n","        # f.write('\\n')\n","        # f.write('\\n')\n","        # f.close()\n","\n","        # np.save(folder_path + 'metrics.npy', np.array([mae, mse, rmse, mape, mspe,rse, corr]))\n","        # np.save(folder_path + 'pred.npy', preds)\n","        # np.save(folder_path + 'true.npy', trues)\n","        # np.save(folder_path + 'x.npy', inputx)\n","        return\n","\n","    def predict(self, setting, load=False):\n","        pred_data, pred_loader = self._get_data(flag='pred')\n","\n","        if load:\n","            path = os.path.join(self.args.checkpoints, setting)\n","            best_model_path = path + '/' + 'checkpoint.pth'\n","            self.model.load_state_dict(torch.load(best_model_path))\n","\n","        preds = []\n","\n","        self.model.eval()\n","        with torch.no_grad():\n","            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(pred_loader):\n","                batch_x = batch_x.float().to(self.device)\n","                batch_y = batch_y.float()\n","                batch_x_mark = batch_x_mark.float().to(self.device)\n","                batch_y_mark = batch_y_mark.float().to(self.device)\n","\n","                # decoder input\n","                dec_inp = torch.zeros([batch_y.shape[0], self.args.pred_len, batch_y.shape[2]]).float().to(batch_y.device)\n","                dec_inp = torch.cat([batch_y[:, :self.args.label_len, :], dec_inp], dim=1).float().to(self.device)\n","                # encoder - decoder\n","                if 'Linear' in self.args.model:\n","                    outputs = self.model(batch_x)\n","                else:\n","                    if self.args.output_attention:\n","                        outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n","                    else:\n","                        outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n","                pred = outputs.detach().cpu().numpy()  # .squeeze()\n","                preds.append(pred)\n","\n","        preds = np.array(preds)\n","        preds = preds.reshape(-1, preds.shape[-2], preds.shape[-1])\n","\n","        # result save\n","        folder_path = './results/' + setting + '/'\n","        if not os.path.exists(folder_path):\n","            os.makedirs(folder_path)\n","\n","        np.save(folder_path + 'real_prediction.npy', preds)\n","\n","        return"],"metadata":{"id":"lmVKVz8LKBfV","executionInfo":{"status":"ok","timestamp":1733261633482,"user_tz":-120,"elapsed":887,"user":{"displayName":"Mantas Bagdonas","userId":"12975773294315109710"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_H{}_{}'.format(\n","            config.model_id,\n","            config.model,\n","            config.data,\n","            config.features,\n","            config.seq_len,\n","            config.label_len,\n","            config.pred_len,\n","            config.H_order, 1)\n","\n","config.features = 'MS'\n","main = M5FITS(config)\n","\n","## S\n","main.train(setting, ft=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dFGBI2KGOrjt","outputId":"6d70aceb-d47b-4e36-9863-82f2c1eec89d","executionInfo":{"status":"ok","timestamp":1733262389482,"user_tz":-120,"elapsed":101020,"user":{"displayName":"Mantas Bagdonas","userId":"12975773294315109710"}}},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Use CPU\n","0 1552\n","1496 1913\n","1885 1969\n","Model(\n","  (freq_upsampler): Linear(in_features=16, out_features=24, bias=True)\n",")\n","[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n","FLOPs:  86016.0\n","params:  408.0\n","Trainable parameters:  408\n","!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!\n","0.0001\n","Epoch: 1 cost time: 0.9846093654632568\n","Epoch: 1, Steps: 45 | Train Loss: 1.1337448 Vali Loss: 0.9233958 Test Loss: 20.3183861\n","Validation loss decreased (inf --> 0.923396).  Saving model ...\n","Updating learning rate to 0.0001\n","Epoch: 2 cost time: 1.1247379779815674\n","Epoch: 2, Steps: 45 | Train Loss: 1.1074700 Vali Loss: 0.9074556 Test Loss: 20.2344322\n","Validation loss decreased (0.923396 --> 0.907456).  Saving model ...\n","Updating learning rate to 9.5e-05\n","Epoch: 3 cost time: 1.435006856918335\n","Epoch: 3, Steps: 45 | Train Loss: 1.0765099 Vali Loss: 0.8863791 Test Loss: 20.1549664\n","Validation loss decreased (0.907456 --> 0.886379).  Saving model ...\n","Updating learning rate to 9.025e-05\n","Epoch: 4 cost time: 0.938126802444458\n","Epoch: 4, Steps: 45 | Train Loss: 1.0545251 Vali Loss: 0.8697476 Test Loss: 20.0816536\n","Validation loss decreased (0.886379 --> 0.869748).  Saving model ...\n","Updating learning rate to 8.573749999999999e-05\n","Epoch: 5 cost time: 0.9503717422485352\n","Epoch: 5, Steps: 45 | Train Loss: 1.0311986 Vali Loss: 0.8558804 Test Loss: 20.0122700\n","Validation loss decreased (0.869748 --> 0.855880).  Saving model ...\n","Updating learning rate to 8.1450625e-05\n","Epoch: 6 cost time: 0.9302928447723389\n","Epoch: 6, Steps: 45 | Train Loss: 1.0121494 Vali Loss: 0.8456665 Test Loss: 19.9478951\n","Validation loss decreased (0.855880 --> 0.845667).  Saving model ...\n","Updating learning rate to 7.737809374999998e-05\n","Epoch: 7 cost time: 0.9517512321472168\n","Epoch: 7, Steps: 45 | Train Loss: 0.9972238 Vali Loss: 0.8336521 Test Loss: 19.8928356\n","Validation loss decreased (0.845667 --> 0.833652).  Saving model ...\n","Updating learning rate to 7.350918906249998e-05\n","Epoch: 8 cost time: 1.191718578338623\n","Epoch: 8, Steps: 45 | Train Loss: 0.9763796 Vali Loss: 0.8187153 Test Loss: 19.8404980\n","Validation loss decreased (0.833652 --> 0.818715).  Saving model ...\n","Updating learning rate to 6.983372960937497e-05\n","Epoch: 9 cost time: 1.4592716693878174\n","Epoch: 9, Steps: 45 | Train Loss: 0.9665509 Vali Loss: 0.8054351 Test Loss: 19.7892208\n","Validation loss decreased (0.818715 --> 0.805435).  Saving model ...\n","Updating learning rate to 6.634204312890623e-05\n","Epoch: 10 cost time: 0.9260613918304443\n","Epoch: 10, Steps: 45 | Train Loss: 0.9515958 Vali Loss: 0.7960774 Test Loss: 19.7429390\n","Validation loss decreased (0.805435 --> 0.796077).  Saving model ...\n","Updating learning rate to 6.30249409724609e-05\n","Epoch: 11 cost time: 0.9313662052154541\n","Epoch: 11, Steps: 45 | Train Loss: 0.9404246 Vali Loss: 0.7880210 Test Loss: 19.7021523\n","Validation loss decreased (0.796077 --> 0.788021).  Saving model ...\n","Updating learning rate to 5.987369392383787e-05\n","Epoch: 12 cost time: 0.9381871223449707\n","Epoch: 12, Steps: 45 | Train Loss: 0.9290737 Vali Loss: 0.7830646 Test Loss: 19.6622601\n","Validation loss decreased (0.788021 --> 0.783065).  Saving model ...\n","Updating learning rate to 5.688000922764597e-05\n","Epoch: 13 cost time: 0.9244310855865479\n","Epoch: 13, Steps: 45 | Train Loss: 0.9164846 Vali Loss: 0.7758490 Test Loss: 19.6262722\n","Validation loss decreased (0.783065 --> 0.775849).  Saving model ...\n","Updating learning rate to 5.403600876626367e-05\n","Epoch: 14 cost time: 1.0960538387298584\n","Epoch: 14, Steps: 45 | Train Loss: 0.9087650 Vali Loss: 0.7672487 Test Loss: 19.5880756\n","Validation loss decreased (0.775849 --> 0.767249).  Saving model ...\n","Updating learning rate to 5.1334208327950485e-05\n","Epoch: 15 cost time: 1.4755213260650635\n","Epoch: 15, Steps: 45 | Train Loss: 0.9005165 Vali Loss: 0.7591494 Test Loss: 19.5585766\n","Validation loss decreased (0.767249 --> 0.759149).  Saving model ...\n","Updating learning rate to 4.876749791155295e-05\n","Epoch: 16 cost time: 1.001460313796997\n","Epoch: 16, Steps: 45 | Train Loss: 0.8936674 Vali Loss: 0.7505158 Test Loss: 19.5278301\n","Validation loss decreased (0.759149 --> 0.750516).  Saving model ...\n","Updating learning rate to 4.6329123015975305e-05\n","Epoch: 17 cost time: 0.9451746940612793\n","Epoch: 17, Steps: 45 | Train Loss: 0.8860613 Vali Loss: 0.7487562 Test Loss: 19.4981480\n","Validation loss decreased (0.750516 --> 0.748756).  Saving model ...\n","Updating learning rate to 4.4012666865176535e-05\n","Epoch: 18 cost time: 0.9581985473632812\n","Epoch: 18, Steps: 45 | Train Loss: 0.8782125 Vali Loss: 0.7436889 Test Loss: 19.4735565\n","Validation loss decreased (0.748756 --> 0.743689).  Saving model ...\n","Updating learning rate to 4.181203352191771e-05\n","Epoch: 19 cost time: 0.923088550567627\n","Epoch: 19, Steps: 45 | Train Loss: 0.8722769 Vali Loss: 0.7396582 Test Loss: 19.4490356\n","Validation loss decreased (0.743689 --> 0.739658).  Saving model ...\n","Updating learning rate to 3.972143184582182e-05\n","Epoch: 20 cost time: 1.200998067855835\n","Epoch: 20, Steps: 45 | Train Loss: 0.8660257 Vali Loss: 0.7315819 Test Loss: 19.4246693\n","Validation loss decreased (0.739658 --> 0.731582).  Saving model ...\n","Updating learning rate to 3.7735360253530726e-05\n","Epoch: 21 cost time: 1.516296625137329\n","Epoch: 21, Steps: 45 | Train Loss: 0.8605597 Vali Loss: 0.7339955 Test Loss: 19.4049797\n","EarlyStopping counter: 1 out of 3\n","Updating learning rate to 3.584859224085419e-05\n","Epoch: 22 cost time: 0.9242634773254395\n","Epoch: 22, Steps: 45 | Train Loss: 0.8540566 Vali Loss: 0.7226759 Test Loss: 19.3858852\n","Validation loss decreased (0.731582 --> 0.722676).  Saving model ...\n","Updating learning rate to 3.405616262881148e-05\n","Epoch: 23 cost time: 0.9373641014099121\n","Epoch: 23, Steps: 45 | Train Loss: 0.8499868 Vali Loss: 0.7298551 Test Loss: 19.3677063\n","EarlyStopping counter: 1 out of 3\n","Updating learning rate to 3.2353354497370904e-05\n","Epoch: 24 cost time: 0.9745922088623047\n","Epoch: 24, Steps: 45 | Train Loss: 0.8459260 Vali Loss: 0.7197365 Test Loss: 19.3508339\n","Validation loss decreased (0.722676 --> 0.719737).  Saving model ...\n","Updating learning rate to 3.073568677250236e-05\n","Epoch: 25 cost time: 0.9813904762268066\n","Epoch: 25, Steps: 45 | Train Loss: 0.8417412 Vali Loss: 0.7197082 Test Loss: 19.3334541\n","Validation loss decreased (0.719737 --> 0.719708).  Saving model ...\n","Updating learning rate to 2.919890243387724e-05\n","Epoch: 26 cost time: 1.3364319801330566\n","Epoch: 26, Steps: 45 | Train Loss: 0.8367073 Vali Loss: 0.7107297 Test Loss: 19.3181458\n","Validation loss decreased (0.719708 --> 0.710730).  Saving model ...\n","Updating learning rate to 2.7738957312183377e-05\n","Epoch: 27 cost time: 1.5226454734802246\n","Epoch: 27, Steps: 45 | Train Loss: 0.8327554 Vali Loss: 0.7123548 Test Loss: 19.3038616\n","EarlyStopping counter: 1 out of 3\n","Updating learning rate to 2.6352009446574204e-05\n","Epoch: 28 cost time: 0.9414176940917969\n","Epoch: 28, Steps: 45 | Train Loss: 0.8324826 Vali Loss: 0.7139831 Test Loss: 19.2902508\n","EarlyStopping counter: 2 out of 3\n","Updating learning rate to 2.5034408974245492e-05\n","Epoch: 29 cost time: 0.9327564239501953\n","Epoch: 29, Steps: 45 | Train Loss: 0.8285362 Vali Loss: 0.7075001 Test Loss: 19.2768002\n","Validation loss decreased (0.710730 --> 0.707500).  Saving model ...\n","Updating learning rate to 2.3782688525533216e-05\n","Epoch: 30 cost time: 0.9498641490936279\n","Epoch: 30, Steps: 45 | Train Loss: 0.8267182 Vali Loss: 0.7061201 Test Loss: 19.2645302\n","Validation loss decreased (0.707500 --> 0.706120).  Saving model ...\n","Updating learning rate to 2.2593554099256555e-05\n","Epoch: 31 cost time: 0.9502606391906738\n","Epoch: 31, Steps: 45 | Train Loss: 0.8221073 Vali Loss: 0.7066888 Test Loss: 19.2528667\n","EarlyStopping counter: 1 out of 3\n","Updating learning rate to 2.146387639429373e-05\n","Epoch: 32 cost time: 1.4065806865692139\n","Epoch: 32, Steps: 45 | Train Loss: 0.8201784 Vali Loss: 0.7023591 Test Loss: 19.2417030\n","Validation loss decreased (0.706120 --> 0.702359).  Saving model ...\n","Updating learning rate to 2.039068257457904e-05\n","Epoch: 33 cost time: 1.4379072189331055\n","Epoch: 33, Steps: 45 | Train Loss: 0.8135873 Vali Loss: 0.6980591 Test Loss: 19.2317371\n","Validation loss decreased (0.702359 --> 0.698059).  Saving model ...\n","Updating learning rate to 1.9371148445850086e-05\n","Epoch: 34 cost time: 0.9485149383544922\n","Epoch: 34, Steps: 45 | Train Loss: 0.8157959 Vali Loss: 0.6915784 Test Loss: 19.2223988\n","Validation loss decreased (0.698059 --> 0.691578).  Saving model ...\n","Updating learning rate to 1.8402591023557583e-05\n","Epoch: 35 cost time: 0.9699530601501465\n","Epoch: 35, Steps: 45 | Train Loss: 0.8141880 Vali Loss: 0.6944966 Test Loss: 19.2142220\n","EarlyStopping counter: 1 out of 3\n","Updating learning rate to 1.74824614723797e-05\n","Epoch: 36 cost time: 0.9911398887634277\n","Epoch: 36, Steps: 45 | Train Loss: 0.8083933 Vali Loss: 0.6986105 Test Loss: 19.2058697\n","EarlyStopping counter: 2 out of 3\n","Updating learning rate to 1.6608338398760718e-05\n","Epoch: 37 cost time: 0.9538784027099609\n","Epoch: 37, Steps: 45 | Train Loss: 0.8091565 Vali Loss: 0.6871527 Test Loss: 19.1970253\n","Validation loss decreased (0.691578 --> 0.687153).  Saving model ...\n","Updating learning rate to 1.577792147882268e-05\n","Epoch: 38 cost time: 1.4717931747436523\n","Epoch: 38, Steps: 45 | Train Loss: 0.8064237 Vali Loss: 0.6912284 Test Loss: 19.1904011\n","EarlyStopping counter: 1 out of 3\n","Updating learning rate to 1.4989025404881546e-05\n","Epoch: 39 cost time: 1.5059566497802734\n","Epoch: 39, Steps: 45 | Train Loss: 0.8031056 Vali Loss: 0.6913873 Test Loss: 19.1828461\n","EarlyStopping counter: 2 out of 3\n","Updating learning rate to 1.4239574134637468e-05\n","Epoch: 40 cost time: 0.9853956699371338\n","Epoch: 40, Steps: 45 | Train Loss: 0.8039356 Vali Loss: 0.6888178 Test Loss: 19.1757812\n","EarlyStopping counter: 3 out of 3\n","Early stopping\n"]},{"output_type":"execute_result","data":{"text/plain":["Model(\n","  (freq_upsampler): Linear(in_features=16, out_features=24, bias=True)\n",")"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qzZhJlHBPpnz","executionInfo":{"status":"ok","timestamp":1733141840528,"user_tz":-120,"elapsed":234,"user":{"displayName":"Mantas Bagdonas","userId":"12975773294315109710"}},"outputId":"c65dd7e1-922c-4c94-e4a9-b421ae8fc060"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["56"]},"metadata":{},"execution_count":86}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9E4UKnwIQmkB","executionInfo":{"status":"ok","timestamp":1733142356553,"user_tz":-120,"elapsed":223,"user":{"displayName":"Mantas Bagdonas","userId":"12975773294315109710"}},"outputId":"90cd01af-544c-44bc-bb73-7a543aa67f6d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1378\n"]},{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{},"execution_count":94}]},{"cell_type":"code","source":[],"metadata":{"id":"B_99KO5YUdSN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"zdYzF7q8U03y"},"execution_count":null,"outputs":[]}]}