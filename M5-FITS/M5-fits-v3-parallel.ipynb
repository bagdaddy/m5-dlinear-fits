{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28203,"status":"ok","timestamp":1734181412944,"user":{"displayName":"Mantas Bagdonas","userId":"12975773294315109710"},"user_tz":-120},"id":"Lf7E8os30bQh","outputId":"deff5c37-2c39-439a-a2da-41f0e994f407"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2963,"status":"ok","timestamp":1734181747741,"user":{"displayName":"Mantas Bagdonas","userId":"12975773294315109710"},"user_tz":-120},"id":"hhAu-Yow1f3X","outputId":"7bdf12dd-1501-4e36-8880-33a1c60ce752"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting thop\n","  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from thop) (2.5.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->thop) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->thop) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->thop) (3.0.2)\n","Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Installing collected packages: thop\n","Successfully installed thop-0.1.1.post2209072238\n"]}],"source":["!pip3 install thop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"idr61nhh1hwo"},"outputs":[],"source":["root_path = '/content/drive/MyDrive/M5-FITS/processed-nonparam/'\n","checkpoints_dir = '/content/drive/MyDrive/M5-thesis/M5-FITS/checkpoints/v3'\n","fits_dir = '/content/drive/MyDrive/FITS'\n","processed_data_dir_base = '/content/drive/MyDrive/M5-A1/2. data/processed/'\n","BASE     = processed_data_dir_base+'grid_part_1.pkl'\n","PRICE    = processed_data_dir_base+'grid_part_2.pkl'\n","CALENDAR = processed_data_dir_base+'grid_part_3.pkl'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hMSgB2jt1seS"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os, sys, gc, time, warnings, pickle, psutil, random\n","\n","os.chdir(fits_dir)\n","from models.FITS import Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yKjFPDK61uTZ"},"outputs":[],"source":["global_df = pd.concat([pd.read_pickle(BASE),\n","                pd.read_pickle(PRICE).iloc[:,2:],\n","                pd.read_pickle(CALENDAR).iloc[:,2:]],\n","                axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QqQC7zDY1wDS"},"outputs":[],"source":["stores = global_df['store_id'].unique()\n","depts = global_df['dept_id'].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_HUQzCVo1xsR"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import os\n","import torch\n","import random\n","from torch.utils.data import Dataset\n","from sklearn.preprocessing import StandardScaler\n","\n","warnings.filterwarnings('ignore')\n","\n","class Config(object):\n","  def __init__(self, args):\n","    #basic config\n","    self.is_training = args.get('is_training', 1)\n","    self.model_id = args.get('model_id', 'test')\n","    self.model = args.get('model', 'Autoformer')\n","    #dataloader\n","    self.data = args.get('data', 'test')\n","    self.train_share = args.get('train_share', 0.7)\n","    self.root_path = args.get('root_path', '/content/drive/MyDrive/M5-FITS/processed-nonparam')\n","    self.data_path = args.get('data_path', 'm5.csv')\n","    self.features = args.get('features', 'S')\n","    self.target = args.get('target', 'sales')\n","    self.freq = args.get('freq', 'd')\n","    self.checkpoints = args.get('checkpoints', '/content/drive/MyDrive/M5-FITS/checkpoints')\n","    #forecasting\n","    self.seq_len = args.get('seq_len', 56)\n","    self.pred_len = args.get('pred_len', 28)\n","    self.label_len = args.get('label_len', 28)\n","    self.individual = args.get('individual', False)\n","    #optimization\n","    self.num_workers = args.get('num_workers', 10)\n","    self.itr = args.get('itr', 2)\n","    self.train_epochs = args.get('train_epochs', 100)\n","    self.batch_size = args.get('batch_size', 32)\n","    self.patience = args.get('patience', 3)\n","    self.learning_rate = args.get('learning_rate', 0.0001)\n","    self.des = args.get('des', 'test')\n","    self.loss = args.get('loss', 'mse')\n","    self.lradj = args.get('lradj', 'type3')\n","    self.use_amp = args.get('use_amp', False)\n","    #GPU\n","    self.use_gpu = args.get('use_gpu', True)\n","    self.gpu = args.get('gpu', 0)\n","    self.use_multi_gpu = args.get('use_multi_gpu', False)\n","    self.devices = args.get('devices', '0,1,2,3')\n","    self.test_flop = args.get('test_flop', False)\n","    #Augmentation\n","    self.aug_method = args.get('aug_method', 'NA')\n","    self.aug_rate = args.get('aug_rate', 0.5)\n","    self.in_batch_augmentation = args.get('in_batch_augmentation', False)\n","    self.in_dataset_augmentation = args.get('in_dataset_augmentation', False)\n","    self.data_size = args.get('data_size', 1)\n","    self.aug_data_size = args.get('aug_data_size', 1)\n","    self.seed = args.get('seed', 2021)\n","    #continue learning\n","    self.testset_div = args.get('testset_div', 2)\n","    self.test_time_train = args.get('test_time_train', False)\n","    #Formers\n","    self.embed = args.get('embed', 'timeF')\n","    self.enc_in = args.get('enc_in', 7)\n","    self.dec_in = args.get('dec_in', 7)\n","    self.c_out = args.get('c_out', 7)\n","    self.d_model = args.get('d_model', 512)\n","    self.n_heads = args.get('n_heads', 8)\n","    self.e_layers = args.get('e_layers', 2)\n","    self.d_layers = args.get('d_layers', 1)\n","    self.d_ff = args.get('d_ff', 2048)\n","    self.moving_avg = args.get('moving_avg', 25)\n","    self.factor = args.get('factor', 1)\n","    self.distil = args.get('distil', True)\n","    self.dropout = args.get('dropout', 0.1)\n","    self.activation = args.get('activation', 'relu')\n","    self.output_attention = args.get('output_attention', False)\n","    self.do_predict = args.get('do_predict', False)\n","\n","    #Flinear\n","    self.train_mode = args.get('train_mode', 0)\n","    self.cut_freq = args.get('cut_freq', 0)\n","    self.base_T = args.get('base_T', 24)\n","    self.H_order = args.get('H_order', 2)\n","\n","    self.use_gpu = True if torch.cuda.is_available() and self.use_gpu else False\n","    cfreq = args.get('cut_freq', 0)\n","    if cfreq == 0:\n","      self.cut_freq = int(self.seq_len // self.base_T + 1) * self.H_order + 10\n","\n","    fix_seed = self.seed\n","    random.seed(fix_seed)\n","    torch.manual_seed(fix_seed)\n","    np.random.seed(fix_seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ezGyZycI1_SB"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","from utils.timefeatures import time_features\n","\n","start_date = pd.Timestamp(\"2011-01-29\")\n","TRAIN_START = 0\n","TRAIN_END = 1941\n","TEST_END = 1969\n","TRAIN_LEN = TRAIN_END - TRAIN_START\n","HORIZON = 28\n","\n","\"\"\"\n","get df_train and features\n","\"\"\"\n","def prepare_features(calendar):\n","  df = calendar.copy()\n","  df['snap_CA'] = df['snap_CA'].astype(int)\n","  df['snap_TX'] = df['snap_TX'].astype(int)\n","  df['snap_WI'] = df['snap_WI'].astype(int)\n","  df['snap_count'] = df[['snap_CA','snap_TX','snap_WI']].apply(np.sum,axis=1)\n","\n","  df['is_event_1'] = [isinstance(x , str)*1 for x in df['event_name_1']]\n","  df['is_event_2'] = [isinstance(x , str)*1 for x in df['event_name_2']]\n","  df['is_event'] = df[['is_event_1', 'is_event_2']].apply(np.sum, axis=1)\n","  #turn into boolean\n","  # df['is_event'] = np.where(df['is_event']>0,1,0)\n","\n","  df.drop(columns=['event_name_1', 'event_name_2', 'snap_CA', 'snap_TX', 'snap_WI', 'is_event_1', 'is_event_2'], inplace=True)\n","  df['date'] = start_date + pd.to_timedelta(df['d'] - 1, unit='D')\n","\n","\n","  return df\n","\n","\"\"\"\n","get df_train and features\n","\"\"\"\n","def get_data_by_store_id(store_id, dept_id):\n","  store_data = global_df[(global_df['store_id'] == store_id) & (global_df['dept_id'] == dept_id) & (global_df['d'] >= TRAIN_START)]\n","\n","  return store_data\n","\n","def aggregate_data(df):\n","  df = df.groupby(['d']).agg({\n","    'sales': 'sum',\n","    'event_name_1': 'first',\n","    'event_name_2': 'first',\n","    'snap_CA': 'first',\n","    'snap_TX': 'first',\n","    'snap_WI': 'first',\n","    'tm_w_end': 'first',\n","    'tm_dw': 'first',\n","    'tm_wm': 'first',\n","    'tm_m': 'first',\n","    'tm_w': 'first',\n","    'sell_price': 'mean',\n","  }).reset_index()\n","\n","  df = prepare_features(df)\n","\n","  return df\n","\n","\n","class Dataset_Custom(Dataset):\n","    def __init__(self, config, df, flag='train', size=None,\n","                 features='S',\n","                 target='OT', scale=True, timeenc=0, freq='h'):\n","        self.args = config\n","        # info\n","        if size == None:\n","            self.seq_len = 24 * 4 * 4\n","            self.label_len = 24 * 4\n","            self.pred_len = 24 * 4\n","        else:\n","            self.seq_len = size[0]\n","            self.label_len = size[1]\n","            self.pred_len = size[2]\n","        # init\n","        assert flag in ['train', 'test', 'val']\n","        type_map = {'train': 0, 'val': 1, 'test': 2}\n","        self.set_type = type_map[flag]\n","\n","        self.features = features\n","        self.target = target\n","        self.scale = scale\n","        self.timeenc = timeenc\n","        self.freq = freq\n","\n","        self.df_raw = df\n","\n","        self.__read_data__()\n","        self.collect_all_data()\n","\n","    def __read_data__(self):\n","        self.scaler = StandardScaler()\n","        #TODO: read bottom-level data. Reproduce similar functionality as in DLinear\n","\n","        '''\n","        df_raw.columns: ['date', ...(other features), target feature]\n","        '''\n","        cols = list(self.df_raw.columns)\n","        cols.remove(self.target)\n","        cols.remove('date')\n","        self.df_raw = self.df_raw[['date'] + cols + [self.target]]\n","        # print(cols)\n","        # num_train = int(len(df_raw) * 0.7)\n","        # num_test = int(len(df_raw) * 0.2)\n","        num_test = HORIZON  # Fixed to the last 28 days\n","        num_train = int((TRAIN_END - TRAIN_START) * self.args.train_share)\n","        num_vali = TRAIN_END - TRAIN_START - num_train - num_test\n","        border1s = [0, num_train - self.seq_len, len(self.df_raw) - num_test - self.seq_len]\n","        border2s = [num_train, num_train + num_vali, len(self.df_raw)]\n","\n","        if self.args.test_time_train:\n","            num_train = int(len(self.df_raw) * 0.9)\n","            border1s = [0, num_train - self.seq_len, len(self.df_raw)]\n","            border2s = [num_train, len(self.df_raw), len(self.df_raw)]\n","\n","        border1 = border1s[self.set_type]\n","        border2 = border2s[self.set_type]\n","\n","        if self.features == 'M' or self.features == 'MS':\n","            cols_data = self.df_raw.columns[1:]\n","            df_data = self.df_raw[cols_data]\n","        elif self.features == 'S':\n","            df_data = self.df_raw[[self.target]]\n","\n","        if self.scale:\n","            train_data = df_data[border1s[0]:border2s[0]]\n","            self.scaler.fit(train_data.values)\n","            # print(self.scaler.mean_)\n","            # exit()\n","            data = self.scaler.transform(df_data.values)\n","        else:\n","            data = df_data.values\n","\n","        df_stamp = self.df_raw[['date']][border1:border2]\n","        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n","        if self.timeenc == 0:\n","            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n","            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n","            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n","            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n","            data_stamp = df_stamp.drop(['date'], 1).values\n","        elif self.timeenc == 1:\n","            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n","            data_stamp = data_stamp.transpose(1, 0)\n","\n","        self.data_x = data[border1:border2]\n","        self.data_y = data[border1:border2]\n","        self.data_stamp = data_stamp\n","        print(border1, border2)\n","\n","    def reload_data(self, x_data, y_data, x_time, y_time):\n","        self.x_data = x_data\n","        self.y_data = y_data\n","        self.x_time = x_time\n","        self.y_time = y_time\n","\n","    def collect_all_data(self):\n","        self.x_data = []\n","        self.y_data = []\n","        self.x_time = []\n","        self.y_time = []\n","        data_len = len(self.data_x) - self.seq_len - self.pred_len + 1\n","        mask_data_len = int((1-self.args.data_size) * data_len) if self.args.data_size < 1 else 0\n","        for i in range(len(self.data_x) - self.seq_len - self.pred_len + 1):\n","            if (self.set_type == 0 and i >= mask_data_len) or self.set_type != 0:\n","                s_begin = i\n","                s_end = s_begin + self.seq_len\n","                r_begin = s_end - self.label_len\n","                r_end = r_begin + self.label_len + self.pred_len\n","                self.x_data.append(self.data_x[s_begin:s_end])\n","                self.y_data.append(self.data_y[r_begin:r_end])\n","                self.x_time.append(self.data_stamp[s_begin:s_end])\n","                self.y_time.append(self.data_stamp[r_begin:r_end])\n","\n","    def __getitem__(self, index):\n","        seq_x = self.x_data[index]\n","        seq_y = self.y_data[index]\n","        return seq_x, seq_y, self.x_time[index], self.y_time[index]\n","\n","    def __len__(self):\n","        return len(self.x_data)\n","\n","    def inverse_transform(self, data):\n","        return self.scaler.inverse_transform(data)\n","\n","def data_provider(args, df, flag):\n","  timeenc = 0 if args.embed != 'timeF' else 1\n","\n","  if flag == 'test':\n","      shuffle_flag = False\n","      drop_last = False # True\n","      batch_size = args.batch_size\n","      freq = args.freq\n","  else:\n","      shuffle_flag = True\n","      drop_last = True\n","      batch_size = args.batch_size\n","      freq = args.freq\n","\n","  data_set = Dataset_Custom(\n","      config=args,\n","      flag=flag,\n","      df=df,\n","      size=[args.seq_len, args.label_len, args.pred_len],\n","      features=args.features,\n","      target=args.target,\n","      timeenc=timeenc,\n","      freq=freq,\n","      scale=True #try with True later\n","  )\n","\n","  data_loader = DataLoader(\n","      data_set,\n","      batch_size=batch_size,\n","      shuffle=shuffle_flag,\n","      num_workers=args.num_workers,\n","      drop_last=drop_last)\n","\n","  return data_set, data_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fnEx5dpV2IO9"},"outputs":[],"source":["from models.FITS import Model\n","from utils.tools import EarlyStopping, adjust_learning_rate, test_params_flop, visual\n","from utils.metrics import metric\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","from utils.augmentations import augmentation\n","import os\n","import time\n","\n","import warnings\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from thop import profile\n","\n","class FITS(object):\n","    def __init__(self, args, setting, df_train):\n","        self.args = args\n","        self.device = self._acquire_device()\n","        self.model = self._build_model().to(self.device)\n","        self.df_train = df_train\n","        self.top_level = self.get_top_level()\n","        self.setting = setting\n","\n","    def get_top_level(self):\n","      \"\"\"\n","      Calculate top level time series by aggregating low level time-series\n","      \"\"\"\n","      data = aggregate_data(self.df_train)\n","\n","      return data\n","\n","    def get_weights(self):\n","      temp = self.df_train.pivot(index='id', columns='d', values='sales')\n","      temp.fillna(0, inplace=True)\n","      w = np.sum(temp.iloc[:, -56:-28].values, axis=1) / sum(self.top_level['sales'].iloc[-56:-28])\n","      w = w.reshape(len(w), 1)\n","\n","      del temp\n","      gc.collect()\n","\n","      return w\n","\n","    def _build_model(self):\n","        raise NotImplementedError\n","        return None\n","\n","    def _acquire_device(self):\n","        if self.args.use_gpu:\n","            os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(\n","                self.args.gpu) if not self.args.use_multi_gpu else self.args.devices\n","            device = torch.device('cuda:{}'.format(self.args.gpu))\n","            print('Use GPU: cuda:{}'.format(self.args.gpu))\n","        else:\n","            device = torch.device('cpu')\n","            print('Use CPU')\n","        return device\n","\n","    def _get_data(self):\n","        pass\n","\n","    def vali(self):\n","        pass\n","\n","    def train(self):\n","        pass\n","\n","    def test(self):\n","        pass\n","\n","class M5FITS(FITS):\n","    def __init__(self, args, setting, df_train):\n","        super(M5FITS, self).__init__(args, setting, df_train)\n","\n","    def _build_model(self):\n","        model = Model(self.args).float()\n","\n","        if self.args.use_multi_gpu and self.args.use_gpu:\n","            model = nn.DataParallel(model, device_ids=self.args.device_ids)\n","\n","        return model\n","\n","    def _get_data(self, flag):\n","        data_set, data_loader = data_provider(self.args, self.get_top_level(), flag)\n","\n","        return data_set, data_loader\n","\n","    def _select_optimizer(self):\n","        model_optim = optim.Adam(self.model.parameters(), lr=self.args.learning_rate)\n","        print('!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!')\n","        print(self.args.learning_rate)\n","        return model_optim\n","\n","    def _select_criterion(self):\n","        criterion = nn.MSELoss()\n","        return criterion\n","\n","    def _get_profile(self, model):\n","        _input=torch.randn(self.args.batch_size, self.args.seq_len, self.args.enc_in).to(self.device)\n","        macs, params = profile(model, inputs=(_input,))\n","        print('FLOPs: ', macs)\n","        print('params: ', params)\n","        return macs, params\n","\n","    def vali(self, vali_data, vali_loader, criterion):\n","        total_loss = []\n","        self.model.eval()\n","        with torch.no_grad():\n","            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):\n","                batch_x = batch_x.float().to(self.device)\n","                batch_y = batch_y.float().to(self.device)[:,-self.args.pred_len:,:]\n","                batch_xy = torch.cat([batch_x, batch_y], dim=1)\n","\n","                batch_x_mark = batch_x_mark.float().to(self.device)\n","                batch_y_mark = batch_y_mark.float().to(self.device)\n","\n","                # decoder input\n","                dec_inp = torch.zeros_like(batch_y[:, -self.args.pred_len:, :]).float()\n","                dec_inp = torch.cat([batch_y[:, :self.args.label_len, :], dec_inp], dim=1).float().to(self.device)\n","                # encoder - decoder\n","                if 'FITS' in self.args.model:\n","                    outputs, low = self.model(batch_x)\n","                elif 'SCINet' in self.args.model:\n","                    outputs = self.model(batch_x)\n","                else:\n","                    if self.args.output_attention:\n","                        outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n","                    else:\n","                        outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n","                f_dim = -1 if self.args.features == 'MS' else 0\n","                outputs = outputs[:, -self.args.pred_len:, f_dim:]\n","                batch_y = batch_y[:, -self.args.pred_len:, f_dim:]\n","\n","                pred = outputs.detach().cpu()\n","                true = batch_y.detach().cpu()\n","\n","                loss = criterion(pred, true)\n","\n","                total_loss.append(loss)\n","        total_loss = np.average(total_loss)\n","        self.model.train()\n","        return total_loss\n","\n","    def train(self, ft=False):\n","        train_data, train_loader = self._get_data(flag='train')\n","        vali_data, vali_loader = self._get_data(flag='val')\n","        test_data, test_loader = self._get_data(flag='test')\n","        print(self.model)\n","        self._get_profile(self.model)\n","        print('Trainable parameters: ', sum(p.numel() for p in self.model.parameters() if p.requires_grad))\n","\n","        path = os.path.join(self.args.checkpoints, self.setting)\n","        if not os.path.exists(path):\n","            os.makedirs(path)\n","\n","        time_now = time.time()\n","\n","        train_steps = len(train_loader)\n","        early_stopping = EarlyStopping(patience=self.args.patience, verbose=True)\n","\n","        model_optim = self._select_optimizer()\n","        criterion = self._select_criterion()\n","\n","        for epoch in range(self.args.train_epochs):\n","            iter_count = 0\n","            train_loss = []\n","\n","            self.model.train()\n","            epoch_time = time.time()\n","\n","            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n","                iter_count += 1\n","                model_optim.zero_grad()\n","\n","                batch_x = batch_x.float().to(self.device)\n","                batch_y = batch_y.float().to(self.device)[:,-self.args.pred_len:,:]\n","                batch_x_mark = batch_x_mark.float().to(self.device)\n","                batch_y_mark = batch_y_mark.float().to(self.device)\n","                # print(batch_x.shape, batch_y.shape)\n","                batch_xy = torch.cat([batch_x, batch_y], dim=1)\n","\n","                # decoder input\n","                dec_inp = torch.zeros_like(batch_y[:, -self.args.pred_len:, :]).float()\n","                dec_inp = torch.cat([batch_y[:, :self.args.label_len, :], dec_inp], dim=1).float().to(self.device)\n","\n","                # encoder - decoder\n","                if 'FITS' in self.args.model:\n","                        outputs, low = self.model(batch_x)\n","                elif 'SCINet' in self.args.model:\n","                        outputs = self.model(batch_x)\n","                else:\n","                    if self.args.output_attention:\n","                        outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n","                    else:\n","                        outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark, batch_y)\n","\n","                # print(outputs.shape,batch_y.shape)\n","                f_dim = -1 if self.args.features == 'MS' else 0\n","                if ft:\n","                    outputs = outputs[:, -self.args.pred_len:, f_dim:]\n","                    batch_y = batch_y[:, -self.args.pred_len:, f_dim:].to(self.device)\n","                    # print(outputs.shape,batch_xy.shape)\n","                    #loss = criterion(outputs, batch_xy)\n","                    loss = criterion(outputs, batch_y)\n","                else:\n","                    outputs = outputs[:, :, f_dim:]\n","                    # batch_y = batch_y[:, -self.args.pred_len:, f_dim:].to(self.device) #???\n","                    loss = criterion(outputs, batch_xy)\n","                train_loss.append(loss.item())\n","\n","                if (i + 1) % 100 == 0:\n","                    print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n","                    speed = (time.time() - time_now) / iter_count\n","                    left_time = speed * ((self.args.train_epochs - epoch) * train_steps - i)\n","                    print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n","                    iter_count = 0\n","                    time_now = time.time()\n","\n","                loss.backward()\n","                model_optim.step()\n","\n","            print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n","            train_loss = np.average(train_loss)\n","            vali_loss = self.vali(vali_data, vali_loader, criterion)\n","            test_loss = self.vali(test_data, test_loader, criterion)\n","\n","            print(\"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}\".format(\n","                epoch + 1, train_steps, train_loss, vali_loss, test_loss))\n","            early_stopping(vali_loss, self.model, path)\n","            if early_stopping.early_stop:\n","                print(\"Early stopping\")\n","                break\n","\n","            adjust_learning_rate(model_optim, epoch + 1, self.args)\n","\n","        best_model_path = path + '/' + 'checkpoint.pth'\n","        torch.save(self.model.state_dict(), best_model_path)\n","        self.model.load_state_dict(torch.load(best_model_path, weights_only=True))\n","\n","        return self.model\n","\n","    def load_saved_model(self):\n","      print('loading model')\n","      path = os.path.join(self.args.checkpoints, self.setting)\n","      path += '/' + 'checkpoint.pth'\n","      state_dict = torch.load(path, weights_only=True)\n","      del state_dict['total_ops']\n","      del state_dict['total_params']\n","\n","      self.model.load_state_dict(state_dict)\n","\n","    def predict_top_level(self, test=0):\n","      pred_set, pred_loader = self._get_data(flag='test')\n","\n","      if test:\n","        self.load_saved_model()\n","\n","      preds = []\n","\n","      self.model.eval()\n","      with torch.no_grad():\n","        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(pred_loader):\n","          batch_x = batch_x.float().to(self.device)\n","          batch_y = batch_y.float()\n","          batch_x_mark = batch_x_mark.float().to(self.device)\n","          batch_y_mark = batch_y_mark.float().to(self.device)\n","\n","          # decoder input\n","          dec_inp = torch.zeros([batch_y.shape[0], self.args.pred_len, batch_y.shape[2]]).float().to(batch_y.device)\n","          dec_inp = torch.cat([batch_y[:, :self.args.label_len, :], dec_inp], dim=1).float().to(self.device)\n","          # encoder - decoder\n","          outputs, low = self.model(batch_x)\n","          f_dim = -1 if self.args.features == 'MS' else 0\n","          outputs_ = outputs[:, -self.args.pred_len:, f_dim:]\n","          pred = outputs_.detach().cpu().numpy()  # .squeeze()\n","          preds.append(pred)\n","\n","        preds = np.array(preds)\n","        preds = preds.reshape(-1, preds.shape[-2], preds.shape[-1])\n","        preds = pred_set.inverse_transform(preds[0])\n","\n","      return preds.flatten()\n","\n","    def predict_bottom_level(self, test=0):\n","      w = self.get_weights()\n","      top_level_preds = self.predict_top_level(test=test)\n","      top_level_preds = top_level_preds.reshape(1,len(top_level_preds))\n","      preds = np.multiply(top_level_preds, w)\n","\n","      item_ids = self.df_train['id'].unique()\n","      res = pd.DataFrame({\n","          'id': item_ids\n","      })\n","\n","      predictions_df = pd.DataFrame(preds, columns=[f'F{i+1}' for i in range(28)])\n","      preds = pd.concat([res['id'], predictions_df], axis=1)\n","\n","      preds.set_index('id', inplace=True)\n","\n","      return preds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bny6covX_DYo"},"outputs":[],"source":["import concurrent.futures\n","import concurrent\n","\n","def train_and_predict(store_id, dept_id, config, sub_name):\n","  try:\n","    print(f\"Training {store_id} + {dept_id}\")\n","\n","    # Setup configuration and get training data\n","    setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_H{}_{}_{}_{}'.format(\n","        config.model_id,\n","        config.model,\n","        config.data,\n","        config.features,\n","        config.seq_len,\n","        config.label_len,\n","        config.pred_len,\n","        config.H_order,\n","        dept_id,\n","        store_id,\n","        1\n","    )\n","\n","    data = get_data_by_store_id(store_id, dept_id)\n","    fits = M5FITS(config, setting=setting, df_train=data)\n","    fits.train()\n","\n","    # Predict bottom-level values\n","    sub_submission = fits.predict_bottom_level()\n","\n","    # Save sub_submission to a temporary CSV file\n","    temp_file_path = f'/tmp/{sub_name}_{store_id}_{dept_id}.csv'\n","    sub_submission.to_csv(temp_file_path)\n","\n","    print(f\"Finished {store_id} + {dept_id}\")\n","\n","    # Collect garbage to free memory\n","    gc.collect()\n","\n","    return temp_file_path\n","\n","  except Exception as e:\n","    print(f\"Error training {store_id} + {dept_id}: {e}\")\n","    return None\n","\n","def parallel_training(config, stores, depts, max_workers=None):\n","    \"\"\"\n","    Main function to parallelize the training of models for each (store_id, dept_id) combination.\n","    \"\"\"\n","    model_name = f\"fits-lb-{config.seq_len}-epochs-100-v3\"\n","    submission = pd.read_csv('/content/drive/MyDrive/data/m5-data/sample_submission.csv')\n","    submission.set_index('id', inplace=True)\n","\n","    all_tasks = []\n","\n","    # Parallel execution\n","    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n","        futures = []\n","\n","        for store_id in stores:\n","            for dept_id in depts:\n","                # Submit each (store_id, dept_id) job to the process pool\n","                # future = executor.submit(train_and_predict, store_id, dept_id, config, model_name)\n","                future = executor.submit(some_function, 2)\n","                futures.append(future)\n","\n","\n","        # Collect the results as each task finishes\n","        for future in concurrent.futures.as_completed(futures):\n","            try:\n","                temp_file_path = future.result()\n","\n","                if temp_file_path and os.path.exists(temp_file_path):\n","                    # Load the individual sub_submission file and merge it into the main submission\n","                    sub_submission = pd.read_csv(temp_file_path, index_col='id')\n","                    submission.update(sub_submission)\n","\n","                    # Delete the temporary file\n","                    os.remove(temp_file_path)\n","\n","            except Exception as e:\n","                print(f\"Error collecting results: {e}\")\n","\n","    # Reset the index of the submission and save it to a file\n","    submission.reset_index(inplace=True)\n","    submission.to_csv(f\"/content/drive/MyDrive/M5-thesis/M5-FITS/submissions/v3/{model_name}.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y1r3__Y7BEbB"},"outputs":[],"source":["cpu_count = min(10, os.cpu_count())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vWxolIIKAbu9"},"outputs":[],"source":["stores = ['CA_1', 'CA_2']\n","depts = ['HOBBIES_1', 'HOBBIES_2']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R2uHAdWhArJi"},"outputs":[],"source":["config = Config({\n","  'features': 'S',\n","  'seed': 42,\n","  'train_share': 0.85,\n","  'model': 'FITS',\n","  'checkpoints': '/content/drive/MyDrive/M5-thesis/M5-FITS/checkpoints/v3',\n","})"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"WavviRN4A_Nj","outputId":"01076b1f-7143-4e84-8d49-a521ffd3554d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training CA_1 + HOBBIES_1\n","Use GPU: cuda:0\n","0 1649\n","1593 1913\n","1885 1969\n","Model(\n","  (freq_upsampler): Linear(in_features=16, out_features=24, bias=True)\n",")\n","[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n","FLOPs:  86016.0\n","params:  408.0\n","Trainable parameters:  408\n","!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!\n","0.0001\n"]}],"source":["tmp = train_and_predict('CA_1', 'HOBBIES_1', config, 'fits-lb-28-epochs-100-v3')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1734182256207,"user":{"displayName":"Mantas Bagdonas","userId":"12975773294315109710"},"user_tz":-120},"id":"oYmjwB_1T504","outputId":"281964b5-620d-4afa-b03e-84e4b41e1260"},"outputs":[{"data":{"text/plain":["<module 'torch' from '/usr/local/lib/python3.10/dist-packages/torch/__init__.py'>"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xj5wpDwyUTXF"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyOTK+AI2iLxHVWwJCta5O30"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}